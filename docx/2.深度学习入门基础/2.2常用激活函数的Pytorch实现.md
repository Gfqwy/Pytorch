在物体检测任务中，常用的激活函数有Sigmoid、 ReLU及Softmax函数
### 1. Sigmoid函数
Sigmoid型函数又称为Logistic函数，当神经元获得的输入信号累计超过一定的阈值后，神经元被激活而处于 兴奋状态，否则处于抑制状态。公式如下：
$$f(x)=\frac{1}{1+e^{-x}}$$
Sigmoid函数曲线与梯度曲线如下图所示。可以看到，Sigmoid函数 将特征压缩到了(0,1)区间，0端对应抑制状态，而1对应激活状态，中间部分梯度较大。
![](/imgs/md110.png)
```python
import torch
from torch import nn
input1 = torch.ones(1,1,2,2)
sigmoid = nn.Sigmoid()
print(sigmoid(input1))

Out[1]
tensor([[[[0.7311, 0.7311],
          [0.7311, 0.7311]]]])
```
Sigmoid函数可以用来做二分类，但其计算量较大，并且容易出现 梯度消失现象。在Sigmoid函数两侧的特征导数接近于0，这将导致在梯度反传时损失的误差难以传递到前面的网络层（因为根据链式求导，梯度接近于0）。

### 2. ReLU函数
为了缓解梯度消失现象，修正线性单元（Rectified Linear Unit， ReLU）被引入到神经网络中。由于其优越的性能与简单优雅的实现，
ReLU已经成为目前卷积神经网络中最为常用的激活函数之一。ReLU函 数的表达式如式（3-2）所示。
$$R(x)=max(0,x)=\left\{
\begin{aligned}
0,  \qquad x\leq0 \\
x ,  \qquad x>0 \\
\end{aligned}
\right.
$$
ReLU函数及其梯度曲线如下图所示。可以看出，在小于0的部分， 值与梯度皆为0，而在大于0的部分中导数保持为1，避免了Sigmoid函数中梯度接近于0导致的梯度消失问题
![](/imgs/md111.png)
```python
import torch
from torch import nn
input1 = torch.ones(1,1,2,2)
#Sigmoid函数
sigmoid = nn.Sigmoid()
# print(sigmoid(input1))

# ReLU函数
relu = nn.ReLU(inplace=True)
print(relu(input1))

Out[2]:
tensor([[[[1., 1.],
          [1., 1.]]]])
```
### 3. Leaky RuLU函数
Leaky ReLU函数是一个ReLU函数的改良版本，它的出现是为了解决“死亡神经元”的问题。公式是：
$$ f(x)=\left\{
\begin{aligned}
\alpha x ,  \qquad x<0 \\
x ,  \qquad x\geq0 \\
\end{aligned}
\right.$$
在负区间内避免了直接置0，而是赋予很小的权重，如0.01，这样可以避免在x<0时，不能够学习的情况，具体见下图：
![](/imgs/md117.png)
```python
input2 = torch.rand(1, 1, 2, 2)
# 利用nn.LeakyReLU()构建激活函数，并且其为0.04，True代表in-place操作
leakyrelu=nn.LeakyReLU(0.04, True)
leakyrelu(input2)

Out[13]: 
tensor([[[[0.9341, 0.3576],
          [0.7528, 0.4066]]]])
```
虽然从理论上讲，Leaky ReLU函数的使用效果应该要比ReLU函数 好，但是从大量实验结果来看并没有看出其效果比ReLU好。

### 4. Softmax函数
softmax函数也是一种sigmoid函数，用于处理多类问题，需要注意的是sigmoid函数一般只能处理两个类，并不适用于多分类的问题，虽然也可以通过构造多个二分类器来解决这个问题，但比较麻烦，。而softmax可以有效解决这个问题。
Softmax函数的输入往往是多个类别的得分，输出则是每一个类别对应的概率，所有类别的概率取值都在0~1之间，且和为1。Softmax函数的表达式如下：
$$\delta_j=\frac{e^{Z_j}}{\sum^K_{k=1}e^{Z_j}}, \quad for \quad j=1,....K$$
其中，$Z_j$表示第$j$个类别的得分，$K$代表分类的类别总数，输出$\delta_j$为第$j$个类别的概率。
```python
import torch.nn.functional as F
score = torch.randn(1,4)
score
Out[14]: tensor([[-1.8252, -1.0366,  0.9608,  1.4270]])

F.softmax(score, 1)
Out[15]: tensor([[0.0221, 0.0486, 0.3583, 0.5710]])
```
