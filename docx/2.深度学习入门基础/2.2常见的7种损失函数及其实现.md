### 1. 什么是损失函数(Loss Function)
损失函数是机器学习算法中的一个重要部分，主要用于进行算法对特征数据集建模效果的评估，衡量算法的性能。

`损失函数是用来估量模型的预测值f(x)与真实值Y的不一致程度`，而`成本函数是所有损失函数的平均值`。但是一般两者语义没有明显的区分。 损失函数直接反映了机器学习模型的预测结果。一般而言，损失函数越小，一般就代表模型的鲁棒性越好，所建立的模型，提供的结果就越好。所以损失函数被用于评估模型的性能，通常人们想要损失函数最小化。
广义地说，损失函数根据应用场景可以分为两大类：`分类问题和回归问题`。在分类问题中，任务是预测问题所处理的所有类的各自概率。相反，在回归问题中，任务是预测一组给定的独立特征对学习算法的连续值。
### 2. 分类任务损失
$y \in \{-1,+1\}$
|Loss|$l(h_w(X_i,yi))$|代表算法|说明|
|--|--|--|--|
|1.Hing-Loss |$max\left[1-h_{\mathbf{w}}(\mathbf{x}_{i})y_{i},0\right]^{p}$|标准SVM(p=1)(Differentiable) Squared Hingeless SVM (p=2)|当用于标准SVM时，损失函数表示线性分隔符与其中任一类中的最近点之间的边距长度。 只有在p = 2时处处可导。   |
||||
||||
#### 2.1 0-1 loss
0-1 loss是最原始的loss，它直接比较输出值与输入值是否相等，对于样本i，它的loss等于：
$$ L(y_i,f(x_i))=\left\{
\begin{aligned}
0  \qquad if \quad y_i= f(x_i)\\
1  \qquad if \quad y_i\not= f(x_i)\\
\end{aligned}
\right.$$
当标签与预测类别相等时，loss为0，否则为1。可以看出，0-1 loss无法对x进行求导，这在依赖于反向传播的深度学习任务中，无法被使用，0-1 loss更多的是启发新的loss的产生。


### 7. 二分类损失函数
### 8. 二分类交叉熵（Binary cross entropy）
### 9. Hinge损失
Hinge损失主要用于带有类标签-1和1的支持向量机(SVM)。因此，请确保将数据集中"恶性"类的标签从0更改为-1。
>Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测。

数据对(x，y)的Hinge损失:$$L=max(0,1-y*f(x))$$
Hinge损失简化了SVM的数学运算，同时最大化了损失(与对数损失(Log-Loss)相比)。`当我们想要做实时决策而不是高度关注准确性时，就可以使用它。`


### 3. 回归损失
#### 3.1 均方误差（MSE，也称L2损失）
$$MSE=\sum^n_{i=1}(y_i-y_i^p)^2$$
均方误差(MSE)是最常用的回归损失函数，计算方法是求预测值与真实值之间距离的平方和
![](/imgs/md10.png)
```python
def mse(true_value, pred_value):
    return np.sum((true_value - pred_value)**2)
```

#### 3.2 平均绝对值误差（MAE，也称L1损失）
MAE是目标值和预测值之差的绝对值之和，MAE也称为L1 Loss：$$ MAE = \sum\limits_{i=1}^n {|y_i - y_i^p|} $$
它只衡量了预测值误差的`平均`模长，而不考虑方向，取值范围也是从0到正无穷（如果考虑方向，则是残差/误差的总和――平均偏差（MBE））
`与MSE相比，MAE成本对异常值更加健壮`
```PYTHON
def mae(true_value, pred_value):
    return np.sum(np.abs(true_value - pred_value))
```
>如果异常点代表在商业中很重要的异常情况，并且需要被检测出来，则应选用MSE损失函数(L2损失函数)。相反，如果只把异常值当作受损数据，则应选用MAE损失函数(L1损失函数)。

>总而言之，处理异常点时，L1损失函数更稳定，但它的导数不连续，因此求解效率较低。L2损失函数对异常点更敏感，但通过令其导数为0，可以得到更稳定的封闭解。

#### 3.3  Huber损失
`Huber损失结合了MSE和MAE的最佳特性`。Huber损失对数据中的异常点没有平方误差损失那么敏感。它在0也可微分。本质上，Huber损失是绝对误差，只是在误差很小时，就变为平方误差。误差降到多小时变为二次误差由超参数δ（delta）来控制。当Huber损失在$[0-δ,0+δ]$之间时，等价为MSE，而在$[-∞,δ]$和$[δ,+∞]$时为MAE。Huber损失需要确定δ参数：
$$ L_δ=\left\{
\begin{aligned}
\frac{1}{2}(y-f(x))^2,  \qquad if \quad |y-f(x)|\leq δ\\
δ|y-f(x)|-\frac{1}{2}δ^2,   \qquad otherwise\\
\end{aligned}
\right.$$
![](/imgs/md12.png)
```python
def sm_mae(true, pred, delta):
    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))
    return np.sum(loss)
```
这里超参数delta的选择非常重要，因为这决定了你对与异常点的定义。当残差大于delta，应当采用L1（对较大的异常值不那么敏感）来最小化，而残差小于超参数，则用L2来最小化。

#### 3.4 Log-Cosh损失
Log-cosh是比L2更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。
$$ L(y, y^p) = \sum\limits_{i=1}^n {\log(\cosh(y_i^p-y_i))} $$
![](/imgs/md13.png)
```python
def logcosh(true, pred):
    loss = np.log(np.cosh(pred - true))
    return np.sum(loss)
```
优点：对于较小的$x$，$log(cosh(x))$近似等于$(x^2)/2$，对于较大的$x$，近似等于$abs(x)-log(2)$。这意味着‘$logcosh$’基本类似于均方误差，但不易受到异常点的影响。它具有Huber损失所有的优点，但不同于Huber损失的是，Log-cosh二阶处处可微。
(*为什么需要二阶导数？许多机器学习模型如XGBoost，就是采用牛顿法来寻找最优点。而牛顿法就需要求解二阶导数（Hessian）。因此对于诸如XGBoost这类机器学习框架，损失函数的二阶可微是很有必要的。但Log-cosh损失也并非完美，其仍存在某些问题。比如误差很大的话，一阶梯度和Hessian会变成定值，这就导致XGBoost出现缺少分裂点的情况。*)

#### 3.5 分位数损失
当我们`更关注区间预测`而不仅是点预测时，分位数损失函数就很有用。使用最小二乘回归进行区间预测，基于的假设：残差（y-y_hat）是独立变量，且方差保持不变。
一旦违背了这条假设，那么线性回归模型就不成立。但是我们也不能因此就认为使用非线性函数或基于树的模型更好，而放弃将线性回归模型作为基线方法。这时，分位数损失和分位数回归就派上用场了，因为即便对于具有变化方差或非正态分布的残差，基于分位数损失的回归也能给出合理的预测区间。
`如何选取合适的分位值取决于我们对正误差和反误差的重视程度`。损失函数通过分位值（γ）对高估和低估给予不同的惩罚。例如，当分位数损失函数$γ=0.25$时，对高估的惩罚更大，使得预测值略低于中值。
$$ L_\gamma(y, y^p) = \sum\limits_{i=y_i<y_i^p} ({\gamma-1})|y_i - y_i^p| + \sum\limits_{i=y_i\geq y_i^p} ({\gamma})|y_i - y_i^p| $$

γ是所需的分位数，其值介于0和1之间。
![](/imgs/md14.png)这个损失函数也可以在神经网络或基于树的模型中计算预测区间。
#### 3.6 对比研究
为了证明上述所有回归损失函数的特点，让我们来一起看一个对比研究。首先，我们建立了一个从sinc（x）函数中采样得到的数据集，并引入了两项人为噪声：高斯噪声分量$ε?N(0，σ2)$和脉冲噪声分量$ξ?Bern(p)$。加入脉冲噪声是为了说明模型的鲁棒效果。以下是使用不同损失函数拟合GBM回归器的结果。
![](https://image.jiqizhixin.com/uploads/editor/25f955e0-efd4-48db-b78c-eee6eb0015a8/1529558777233.png)
连续损失函数：
（A）MSE损失函数(L2损失)；
（B）MAE损失函数(L1损失)；
（C）Huber损失函数；
（D）分位数损失函数。将一个平滑的GBM拟合成有噪声的sinc（x）数据的示例：
（E）原始sinc（x）函数；
（F）具有MSE和MAE损失的平滑GBM；
（G）具有Huber损失的平滑GBM，且$δ=\{4,2,1\}$；
（H）具有分位数损失的平滑的GBM，且$α=\{0.5,0.1,0.9\}$。
`仿真对比的一些观察结果`：
* MAE损失模型的预测结果受脉冲噪声的影响较小，而MSE损失函数的预测结果受此影响略有偏移。
* Huber损失模型预测结果对所选超参数不敏感。
* 分位数损失模型在合适的置信水平下能给出很好的估计。

#### 3.7 总结：将所有回归损失函数都放进一张图
![](/imgs/md15.png)

### 参考文献
[机器学习大牛最常用的5个回归损失函数，你知道几个？](https://www.jiqizhixin.com/articles/2018-06-21-3)
[机器学习损失函数的代码实现](https://nbviewer.jupyter.org/github/groverpr/Machine-Learning/blob/master/notebooks/05_Loss_Functions.ipynb)
[损失函数总结](https://www.cnblogs.com/nxf-rabbit75/p/10440805.html#auto-id-1)