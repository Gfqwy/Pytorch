PyTorch提供了自动求导机制autograd，将前向传播的计算记录成计算图，自动完成求导。
自动求导机制记录了Tensor的操作，以便自动求导与反向传播。可以通过requires_grad参数来创建支持自动求导机制的Tensor。`require_grad参数表示是否需要对该Tensor进行求导，默认为False； 设置为True则需要求导，并且依赖于该Tensor的之后的所有节点都需要求导。`
>grad：该Tensor对应的梯度，类型为Tensor，并与Tensor同维度。 >grad_fn：指向function对象，即该Tensor经过了什么样的操作，用作反向传播的梯度计算，`如果该Tensor由用户自己创建，则该grad_fn为None`。
```python
a=torch.randn(2,2,requires_grad=True)
b = torch.randn(2,2)

a.requires_grad, b.requires_grad
Out[78]: (True, False)
# 从输出可以看到默认的Tensor是不需要求导的，设置requires_grad为True后则需要求导

# 通过内置函数requires_grad_()将Tensor变为需要求导
b.requires_grad_()
Out[79]: 
tensor([[-0.2433, -0.9743],
        [ 1.7224,  0.8741]], requires_grad=True)

b.requires_grad
Out[80]: True

# 通过计算生成的Tensor，由于依赖的Tensor需要求导，因此c也需要求导
c = a + b

c.requires_grad
Out[82]: True

# a与b是自己创建的，grad_fn为None，而c的grad_fn则是一个Add函数操作
a.grad_fn, b.grad_fn, c.grad_fn
Out[83]: (None, None, <ThAddBackward at 0xb850390>)

d = c.detach()  # Tensor.detach()函数生成的数据默认requires_grad为False。

d
Out[85]: 
tensor([[ 0.8080, -0.7129],
        [ 1.2773,  0.0777]])

c
Out[86]: 
tensor([[ 0.8080, -0.7129],
        [ 1.2773,  0.0777]], grad_fn=<ThAddBackward>)

d.requires_grad
Out[87]: False
```
PyTorch的Autograd机制会根据计算过程来自动生成`动态图`，然后可以根据动态图的创建过程进行反向传播(backward()函数)，计算得到每个节点的梯度值。
为了能够记录张量的梯度，首先需要在创建张量的时候设置一个`参数requires_grad=True`, 意味着这个张量将会加入到计算图中，作为计算图的叶子节点参与计算，通过一系列的计算，最后输出结果张量，也就是根节点。一旦指定了这个参数，在后续的计算中得到的中间结果的张量都会被设置成requires_grad=True。对于PyTorch来说，每个张量都有一个`grad.fn方法`，这个方法包含着创建该张量的运算的导数信息。在反向传播过程中，通过传入后一层的神经网络的梯度，该函数会计算出参与运算的所有张量的梯度。`grad_fn本身也携带着计算图的信息，该方法本身有一个next_functions属性`，包含连接该张量的其他张量的grad_fn。 通过不断反向传播回溯中间张量的计算节点，可以得到所有张量的梯度。一个张量的梯度张量的信息保存在该张量的grad属性中。除PyTorch张量本身外，PyTorch提供了一一个专门用来做`自动求导的包，即torch.autograd`。它包含有两个重要的函数，即`torch.autograd.backward函数`和`torch.autograd.grad函数`。
torch.autograd.backward函数通过传入根节点张量，以及初始梯度张量(形状和当前张量的相同)，可以计算产生该根节点所有对应的叶子节点的梯度。当张量为标量张量时(Scalar,即只有一个元素的张量)，可以不传入初始梯度张量，默认会设置初始梯度张量为1。当计算梯度张量的时候，原先建立起来的计算图会被自动释放，如果需要再次做自动导，因为计算图已经不存在，就会报错。如果要在反向传播的时候保留计算图，可以设置retain graph=True。另外，在自动求导的时
候默认不会建立反向传播的计算图(因为反向传播也是一个计算过程，可以动态创建计算图)，如果需要在反向传播计算的同时建立和梯度张量相关的计算图(在某些情况下，如需要计算高阶导数的情况下，不过这种情况比较少)，可以设置`create_graph=True`。 对于一个可求导的张量，也可以直接调用该张量内部的backward方法来进行自动求导。
```python
t1 = torch.randn(3, 3, requires_grad=True) # 定义一个3×3的张量

t1
Out[63]: 
tensor([[ 0.0338,  0.8590, -0.3857],
        [ 0.5571,  1.6432,  0.3478],
        [ 0.3852, -0.3185, -0.2896]], requires_grad=True)

t2 = t1.pow(2).sum()  # f(x) = x^2  计算张量的所有分量平方和

t2
Out[65]: tensor(4.3528, grad_fn=<SumBackward0>)

t2.backward() # 反向传播

t2
Out[67]: tensor(4.3528, grad_fn=<SumBackward0>)

t1.grad # f(x)' = 2x  梯度是张量原始分量的2倍
Out[68]: 
tensor([[ 0.0677,  1.7179, -0.7713],
        [ 1.1142,  3.2864,  0.6955],
        [ 0.7704, -0.6370, -0.5791]])

t2 = t1.pow(2).sum() # 再次计算所有分量的平方和

t2
Out[70]: tensor(4.3528, grad_fn=<SumBackward0>)

t2.backward() # 再次反向传播

t1.grad  # 梯度累积
Out[72]: 
tensor([[ 0.1354,  3.4359, -1.5426],
        [ 2.2283,  6.5729,  1.3910],
        [ 1.5408, -1.2740, -1.1583]])

t1   # t1始终没有改变，有改变的是t1.grad
Out[73]: 
tensor([[ 0.0338,  0.8590, -0.3857],
        [ 0.5571,  1.6432,  0.3478],
        [ 0.3852, -0.3185, -0.2896]], requires_grad=True)

t1.grad.zero_() # 单个张量清零梯度的方法
Out[74]: 
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
```
需要注意的一点是,张量绑定的梯度张量在不清空的情况下会逐渐累积。这种特性在某些情况下是有用的，比如,需要一次性求很多迷你批次的累积梯度, 但在一般情况下,不需要用到这个特性，所以`要注意将张量的梯度清零`
(模块和优化器都有清零参数张量梯度的函数）
