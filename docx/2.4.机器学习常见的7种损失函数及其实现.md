### 1. 什么是损失函数(Loss Function)
损失函数是机器学习算法中的一个重要部分，主要用于进行算法对特征数据集建模效果的评估，衡量算法的性能。

`损失函数是用来估量模型的预测值f(x)与真实值Y的不一致程度`，而`成本函数是所有损失函数的平均值`。但是一般两者语义没有明显的区分。 损失函数直接反映了机器学习模型的预测结果。一般而言，损失函数越小，一般就代表模型的鲁棒性越好，所建立的模型，提供的结果就越好。所以损失函数被用于评估模型的性能，通常人们想要损失函数最小化。
广义地说，损失函数根据应用场景可以分为两大类：分类问题和回归问题。在分类问题中，任务是预测问题所处理的所有类的各自概率。相反，在回归问题中，任务是预测一组给定的独立特征对学习算法的连续值。
### 2. 分类任务损失
#### 2.1 0-1 loss
0-1 loss是最原始的loss，它直接比较输出值与输入值是否相等，对于样本i，它的loss等于：
$$ L(y_i,f(x_i))=\left\{
\begin{aligned}
0  \qquad if \quad y_i= f(x_i)\\
1  \qquad if \quad y_i\not= f(x_i)\\
\end{aligned}
\right.$$
当标签与预测类别相等时，loss为0，否则为1。可以看出，0-1 loss无法对x进行求导，这在依赖于反向传播的深度学习任务中，无法被使用，0-1 loss更多的是启发新的loss的产生。

### 3. 回归损失函数
$$Y=a_0+a_1X_1+a_2X_2+...+a_nX_n$$
### 4. 平方误差损失
每个训练样本的平方误差损失(也称为L2 Loss)是实际值和预测值之差的平方：$$L=(y-f(x))^2$$相应的成本函数是这些平方误差的平均值(MSE)。MSE损失函数，它是一个二次函数(形式为$ax^2+bx+c$)，并且值大于等于0。二次函数的图形如下图所示：
![](../imgs/md9.jpg)
二次函数仅具有全局最小值。由于没有局部最小值，所以我们永远不会陷入它。因此，可以保证梯度下降将收敛到全局最小值(如果它完全收敛)。
MSE损失函数通过`平方误差`来惩罚模型犯的大错误。把一个比较大的数平方会使它变得更大。但有一点需要注意，`这个属性使MSE成本函数对异常值的健壮性降低`。`因此，如果我们的数据容易出现许多的异常值，则不应使用这个它。`
### 5. 绝对误差损失
每个训练样本的绝对误差是预测值和实际值之间的距离，与符号无关。绝对误差也称为L1 Loss：$$l=|y-f(X)|$$
成本则是这些绝对误差的平均值(MAE)
`与MSE相比，MAE成本对异常值更加健壮`，但是，在数学方程中处理绝对或模数运算符并不容易

### 6. Huber损失
`Huber损失结合了MSE和MAE的最佳特性`。对于较小的误差，它是二次的，否则是线性的(对于其梯度也是如此)。Huber损失需要确定δ参数：
$$ L_δ=\left\{
\begin{aligned}
\frac{1}{2}(y-f(x))^2,  \qquad if \quad |y= f(x)|\leq δ\\
δ|y-f(x)|-\frac{1}{2}δ^2,   \qquad otherwise\\
\end{aligned}
\right.$$
### 7. 二分类损失函数
### 8. 二分类交叉熵
### 9. Hinge损失
Hinge损失主要用于带有类标签-1和1的支持向量机(SVM)。因此，请确保将数据集中"恶性"类的标签从0更改为-1。
>Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测。

数据对(x，y)的Hinge损失:$$L=max(0,1-y*f(x))$$
Hinge损失简化了SVM的数学运算，同时最大化了损失(与对数损失(Log-Loss)相比)。`当我们想要做实时决策而不是高度关注准确性时，就可以使用它。`
### 10. 多分类损失函数
### 11. 多分类交叉熵损失
### 12. KL散度(Kullback Leibler Divergence Loss)

